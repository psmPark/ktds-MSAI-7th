import streamlit as st
import os
import json
import logging
import re
from dotenv import load_dotenv

# OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install openai)
from openai import AzureOpenAI

# Azure Search ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install azure-search-documents)
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType, VectorizedQuery
from azure.core.credentials import AzureKeyCredential

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.WARNING, format="%(asctime)s - %(levelname)s - %(message)s"
)

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ======================================================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œì»¬ .env íŒŒì¼ì—ì„œ ë¡œë“œ)
# ======================================================================
OPENAI_ENDPOINT = os.getenv("OPENAI_ENDPOINT")
OPENAI_KEY = os.getenv("OPENAI_KEY")
OPENAI_DEPLOYMENT_MODEL = os.getenv("OPENAI_DEPLOYMENT_MODEL")
OPENAI_DEPLOYMENT_EMBEDDING = os.getenv("OPENAI_DEPLOYMENT_EMBEDDING")
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_INDEX_NAME_RULES = os.getenv("AZURE_SEARCH_INDEX_NAME_RULES")
AZURE_SEARCH_INDEX_NAME_QA = os.getenv("AZURE_SEARCH_INDEX_NAME_QA")
AZURE_SEARCH_INDEX_NAME_DICT = os.getenv("AZURE_SEARCH_INDEX_NAME_DICT")

# logging.info(f"OPENAI_ENDPOINT: {OPENAI_ENDPOINT}")
# logging.info(f"OPENAI_KEY: {OPENAI_KEY}")
# logging.info(f"OPENAI_DEPLOYMENT_MODEL: {OPENAI_DEPLOYMENT_MODEL}")
# logging.info(f"OPENAI_DEPLOYMENT_EMBEDDING: {OPENAI_DEPLOYMENT_EMBEDDING}")
# logging.info(f"AZURE_SEARCH_ENDPOINT: {AZURE_SEARCH_ENDPOINT}")
# logging.info(f"AZURE_SEARCH_API_KEY: {AZURE_SEARCH_API_KEY}")
# logging.info(f"AZURE_SEARCH_INDEX_NAME_RULES: {AZURE_SEARCH_INDEX_NAME_RULES}")
# logging.info(f"AZURE_SEARCH_INDEX_NAME_QA: {AZURE_SEARCH_INDEX_NAME_QA}")
# logging.info(f"AZURE_SEARCH_INDEX_NAME_DICT: {AZURE_SEARCH_INDEX_NAME_DICT}")

# 1. í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì‚¬
if not all(
    [
        OPENAI_ENDPOINT,
        OPENAI_KEY,
        AZURE_SEARCH_ENDPOINT,
        AZURE_SEARCH_API_KEY,
        OPENAI_DEPLOYMENT_EMBEDDING,
    ]
):
    logging.error(
        "í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ (OpenAI ë˜ëŠ” Search)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤."
    )
    exit()

# 2. OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=OPENAI_KEY,
    azure_endpoint=OPENAI_ENDPOINT,
    api_version="2024-12-01-preview",
)

# 3. Azure Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
search_credential = AzureKeyCredential(AZURE_SEARCH_API_KEY)
search_client_rules = SearchClient(
    endpoint=AZURE_SEARCH_ENDPOINT,
    index_name=AZURE_SEARCH_INDEX_NAME_RULES,
    credential=search_credential,
)
search_client_qa = SearchClient(
    endpoint=AZURE_SEARCH_ENDPOINT,
    index_name=AZURE_SEARCH_INDEX_NAME_QA,
    credential=search_credential,
)
search_client_dict = SearchClient(
    endpoint=AZURE_SEARCH_ENDPOINT,
    index_name=AZURE_SEARCH_INDEX_NAME_DICT,
    credential=search_credential,
)


# ======================================================================
# RAG íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ (ì´ì „ Function App ë¡œì§ê³¼ ë™ì¼)
# ======================================================================


# ğŸ’¡ ì„ë² ë”© ìƒì„± í•¨ìˆ˜ ì¶”ê°€
def generate_embedding(text: str) -> list[float]:
    """í…ìŠ¤íŠ¸ë¥¼ Azure OpenAI text-embedding-3-small ëª¨ë¸ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    try:
        response = openai_client.embeddings.create(
            input=text, model=OPENAI_DEPLOYMENT_EMBEDDING
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"ì„ë² ë”© API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []


# 1. í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords_with_llm(user_request: str) -> tuple[list, str]:
    """GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì™€ ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì—ì„œ ëª…ëª… ê·œì¹™ ë° ìš©ì–´ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œ(Key Term)ì™€ ì¹´í…Œê³ ë¦¬(Java, Database, WebUI)ë¥¼
        ìµœëŒ€ 5ê°œê¹Œì§€ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”. ìš”ì²­: {user_request} ->
        """
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant for keyword extraction.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
        )
        keywords_str = response.choices[0].message.content.strip()
        keywords_list = [k.strip() for k in keywords_str.split(",") if k.strip()]
        search_query = " OR ".join(keywords_list)
        return keywords_list, search_query
    except Exception as e:
        logging.error(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [user_request], user_request


# 2. ìš©ì–´ì‚¬ì „ ê²€ìƒ‰ í•¨ìˆ˜ (Dictionary Index) - ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì •
def search_dictionary_for_terms(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(í‚¤ì›Œë“œ + ë²¡í„°)ì„ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì‚¬ì „ ì¸ë±ìŠ¤ì—ì„œ ì •ì˜ëœ ìš©ì–´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        # 1. ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = generate_embedding(user_request)

        if not query_vector:
            logging.warning("ì„ë² ë”© ë²¡í„° ìƒì„± ì‹¤íŒ¨. í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            vector_queries = []
        else:
            # 2. VectorizedQuery ê°ì²´ ìƒì„±
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,  # ê²€ìƒ‰í•  Kê°œì˜ ì´ì›ƒ ìˆ˜
                    fields="vector_embedding",  # ì¸ë±ìŠ¤ ë‚´ ë²¡í„° í•„ë“œ ì´ë¦„
                    exhaustive=False,  # HNSW ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
                )
            ]

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (search_textì™€ vector_queries ë™ì‹œ ì‚¬ìš©)
        results = search_client_dict.search(
            search_text=search_query,  # â¬…ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (FULL Lucene)
            vector_queries=vector_queries,  # â¬…ï¸ ë²¡í„° ê²€ìƒ‰
            select=["korean", "english", "abbreviation", "description"],
            top=5,  # ìƒìœ„ 5ê°œ ê²°ê³¼ ë°˜í™˜
            query_type=QueryType.FULL,  # í‚¤ì›Œë“œ ê²€ìƒ‰ íƒ€ì…
        )

        dictionary_context = []
        for result in results:
            # Scoreë¥¼ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ í™•ì¸ ê°€ëŠ¥
            context = f"[Context: ìš©ì–´ì‚¬ì „(Score:{result['@search.score']:.2f})] **í•œêµ­ì–´**: {result['korean']} **ì˜ë¬¸**: {result['english']} **ì•½ì–´**: {result['abbreviation']} **ì„¤ëª…**: {result['description']}"
            dictionary_context.append(context)
        return dictionary_context

    except Exception as e:
        logging.error(f"Azure AI Search (Dictionary Hybrid) ì˜¤ë¥˜: {e}")
        return []


# 3. ëª…ëª… ê·œì¹™ Context ê²€ìƒ‰ í•¨ìˆ˜ (Rules Index) - ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì •
def search_rules_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(í‚¤ì›Œë“œ + ë²¡í„°)ì„ ì‚¬ìš©í•˜ì—¬ ëª…ëª… ê·œì¹™ ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        # 1. ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = generate_embedding(user_request)

        if not query_vector:
            logging.warning("Rules: ì„ë² ë”© ë²¡í„° ìƒì„± ì‹¤íŒ¨. í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            vector_queries = []
        else:
            # 2. VectorizedQuery ê°ì²´ ìƒì„±
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,  # ê²€ìƒ‰í•  Kê°œì˜ ì´ì›ƒ ìˆ˜ (ìµœëŒ€ 5ê°œ)
                    fields="vector_embedding",  # ì¸ë±ìŠ¤ ë‚´ ë²¡í„° í•„ë“œ ì´ë¦„
                    exhaustive=False,
                )
            ]

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (search_textì™€ vector_queries ë™ì‹œ ì‚¬ìš©)
        results = search_client_rules.search(
            search_text=search_query,  # â¬…ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰
            vector_queries=vector_queries,  # â¬…ï¸ ë²¡í„° ê²€ìƒ‰
            select=["category", "type", "rule_en", "rule_kr", "example"],
            top=5,  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¡°ê¸ˆ ë” ëŠ˜ë ¤ Context í™•ë³´ (ê¸°ì¡´ 3ê°œì—ì„œ 5ê°œë¡œ ë³€ê²½)
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            context = f"[Context: {result['category']} {result['type']} Rule (Score:{result['@search.score']:.2f})] **ê·œì¹™**: {result['rule_kr']} **ì˜ˆì‹œ**: {', '.join(result['example'])}"
            context_list.append(context)
        return context_list

    except Exception as e:
        logging.error(f"Azure AI Search (Rules Hybrid) ì˜¤ë¥˜: {e}")
        return []


# 4. Q&A Context ê²€ìƒ‰ í•¨ìˆ˜ (QA Index) - ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì •
def search_qa_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(í‚¤ì›Œë“œ + ë²¡í„°)ì„ ì‚¬ìš©í•˜ì—¬ Q&A ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        # 1. ì„ë² ë”© ë²¡í„° ìƒì„±
        query_vector = generate_embedding(user_request)

        if not query_vector:
            logging.warning("QA: ì„ë² ë”© ë²¡í„° ìƒì„± ì‹¤íŒ¨. í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            vector_queries = []
        else:
            # 2. VectorizedQuery ê°ì²´ ìƒì„±
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=3,  # ê²€ìƒ‰í•  Kê°œì˜ ì´ì›ƒ ìˆ˜ (ìµœëŒ€ 3ê°œ)
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        results = search_client_qa.search(
            search_text=search_query,  # â¬…ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰
            vector_queries=vector_queries,  # â¬…ï¸ ë²¡í„° ê²€ìƒ‰
            select=["category", "question", "answer"],
            top=3,  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¡°ê¸ˆ ë” ëŠ˜ë ¤ Context í™•ë³´ (ê¸°ì¡´ 2ê°œì—ì„œ 3ê°œë¡œ ë³€ê²½)
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            context = f"[Context: QA-{result['category']} (Score:{result['@search.score']:.2f})] **ì§ˆë¬¸**: {result['question']} **ë‹µë³€**: {result['answer']}"
            context_list.append(context)
        return context_list

    except Exception as e:
        logging.error(f"Azure AI Search (QA Hybrid) ì˜¤ë¥˜: {e}")
        return []


# 5. ìµœì¢… ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (Generation)
def generate_response_with_llm(user_request: str, context_list: list) -> str:
    """ê²€ìƒ‰ëœ Contextë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context_str = "\n".join(context_list)

    system_prompt = f"""
    ë‹¹ì‹ ì€ ì½”ë”© ëª…ëª… ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ìƒˆë¡œìš´ ë³€ìˆ˜ëª…ì´ë‚˜ í•¨ìˆ˜ëª…ì„ ìƒì„±í•˜ê³ ,
    ëª…ëª… ê·œì¹™ ë˜ëŠ” ìš©ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    **ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‚¬í•­:**
    1. ì•„ë˜ 'ê²€ìƒ‰ëœ ê·œì¹™', 'ìš©ì–´ì‚¬ì „', 'Q&A'ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    2. ìƒˆë¡œìš´ ëª…ëª…ì„ ìƒì„±í•  ê²½ìš°, í•´ë‹¹ë˜ëŠ” ê·œì¹™(ì˜ˆ: camelCase, PascalCase, ë™ì‚¬ ì‹œì‘ ë“±)ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    3. ìš©ì–´ì‚¬ì „ì—ì„œ ì°¾ì€ ì˜ë¬¸ ëŒ€ì•ˆì´ë‚˜ ì•½ì–´ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±ëœ ìš©ì–´ì˜ ëª…í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    4. Q&A ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì°¾ì•˜ë‹¤ë©´, í•´ë‹¹ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

    **ê²€ìƒ‰ëœ ê·œì¹™ ë° ìš©ì–´ (Context):**
    ---
    {context_str}
    ---
    """

    try:
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_request},
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"OpenAI ìµœì¢… ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})"


# ----------------------------------------------------------------------
# ğŸŒ ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ (í…ŒìŠ¤íŠ¸ ì‹¤í–‰)
# ----------------------------------------------------------------------
if __name__ == "__main__":

    # --- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ ---
    test_requests = [
        # "Webì—ì„œ ë“œë¡­ë‹¤ìš´ ê·œì¹™ì€ ë­ì•¼?",
        "Webì—ì„œ 'ì¶œë°œì§€'ì„ í‘œì‹œí•˜ëŠ” ë¼ë²¨ì€ ë­ë¼ê³  í•´ì•¼í•´?",
        # "Webì—ì„œ 'ê³„ì•½ ì¡°ê±´ì— ë™ì˜' ì²´í¬ë°•ìŠ¤ë¥¼ ëª…ëª…í•˜ëŠ” ì˜ˆì‹œëŠ”?",
    ]

    print("========================================================")
    print("      âœ… extract_keywords_with_llm í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘      ")
    print("========================================================")

    for i, req in enumerate(test_requests):
        print(f"\n--- TEST CASE {i+1} ---")
        print(f"INPUT: {req}")

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords_list, search_query = extract_keywords_with_llm(req)

        # 2. Context ê²€ìƒ‰ (3ê°€ì§€ ì†ŒìŠ¤)
        rules_context = search_rules_for_context(req, search_query)
        dictionary_context = search_dictionary_for_terms(req, search_query)
        qa_context = search_qa_for_context(req, search_query)

        # 3. ëª¨ë“  Context í†µí•©
        all_context = rules_context + dictionary_context + qa_context
        if not all_context:
            final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ëª…ëª… ê·œì¹™ì´ë‚˜ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ êµ¬ì„±ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ ë³´ì„¸ìš”."
        else:
            # 4. ìµœì¢… ì‘ë‹µ ìƒì„± (Generation)
            final_answer = generate_response_with_llm(req, all_context)
        print(f"FINAL ANSWER:\n{final_answer}")
        print("\n========================================================")
        print("                í…ŒìŠ¤íŠ¸ ì™„ë£Œ                           ")
        print("========================================================")
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
