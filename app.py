import streamlit as st
import os
import logging
from dotenv import load_dotenv

# Azure ì„œë¹„ìŠ¤ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)
from openai import AzureOpenAI
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType, VectorizedQuery
from azure.core.credentials import AzureKeyCredential

# ======================================================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ======================================================================
load_dotenv()
logging.basicConfig(
    level=logging.WARNING, format="%(asctime)s - %(levelname)s - %(message)s"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
OPENAI_ENDPOINT = os.getenv("OPENAI_ENDPOINT")
OPENAI_KEY = os.getenv("OPENAI_KEY")
OPENAI_DEPLOYMENT_MODEL = os.getenv("OPENAI_DEPLOYMENT_MODEL")
OPENAI_DEPLOYMENT_EMBEDDING = os.getenv("OPENAI_DEPLOYMENT_EMBEDDING")
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_INDEX_NAME_RULES = os.getenv("AZURE_SEARCH_INDEX_NAME_RULES")
AZURE_SEARCH_INDEX_NAME_QA = os.getenv("AZURE_SEARCH_INDEX_NAME_QA")
AZURE_SEARCH_INDEX_NAME_DICT = os.getenv("AZURE_SEARCH_INDEX_NAME_DICT")
VECTOR_PROFILE_NAME = "qa-vector-profile"

# 1. í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì‚¬
if not all(
    [
        OPENAI_ENDPOINT,
        OPENAI_KEY,
        AZURE_SEARCH_ENDPOINT,
        AZURE_SEARCH_API_KEY,
        OPENAI_DEPLOYMENT_EMBEDDING,
    ]
):
    st.error(
        "í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ (OpenAI/Search)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤."
    )
    st.stop()

# 2. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = AzureOpenAI(
        api_key=OPENAI_KEY,
        azure_endpoint=OPENAI_ENDPOINT,
        api_version="2024-12-01-preview",
    )
    search_credential = AzureKeyCredential(AZURE_SEARCH_API_KEY)

    search_client_rules = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_RULES, search_credential
    )
    search_client_qa = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_QA, search_credential
    )
    search_client_dict = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_DICT, search_credential
    )

except Exception as e:
    st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()


# ======================================================================
# RAG íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ======================================================================


def generate_embedding(text: str) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ Azure OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        response = openai_client.embeddings.create(
            input=text, model=OPENAI_DEPLOYMENT_EMBEDDING
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"ì„ë² ë”© API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []


def extract_keywords_with_llm(user_request: str) -> tuple:
    """GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì™€ ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì—ì„œ ëª…ëª… ê·œì¹™ ë° ìš©ì–´ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œ(Key Term)ì™€ ì¹´í…Œê³ ë¦¬(Java, Database, WebUI)ë¥¼
        ìµœëŒ€ 5ê°œê¹Œì§€ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”. ìš”ì²­: {user_request} ->
        """
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant for keyword extraction.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
        )
        keywords_str = response.choices[0].message.content.strip()
        keywords_list = [k.strip() for k in keywords_str.split(",") if k.strip()]
        search_query = " OR ".join(keywords_list)
        return keywords_list, search_query
    except Exception as e:
        logging.error(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [user_request], user_request


def search_dictionary_for_terms(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì‚¬ì „ ì¸ë±ìŠ¤ì—ì„œ ì •ì˜ëœ ìš©ì–´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        results = search_client_dict.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["korean", "english", "abbreviation", "description"],
            top=5,
            query_type=QueryType.FULL,
        )

        dictionary_context = []
        for result in results:
            score = result.get("@search.score") or 0.0
            context = f"[Context: ìš©ì–´ì‚¬ì „(Score:{score:.2f})] **í•œêµ­ì–´**: {result.get('korean', 'N/A')} **ì˜ë¬¸**: {result.get('english', 'N/A')} **ì•½ì–´**: {result.get('abbreviation', 'N/A')} **ì„¤ëª…**: {result.get('description', 'N/A')}"
            dictionary_context.append(context)
        return dictionary_context
    except Exception as e:
        logging.error(f"Azure AI Search (Dictionary Hybrid) ì˜¤ë¥˜: {e}")
        return []


def search_rules_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ëª…ëª… ê·œì¹™ ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        results = search_client_rules.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["category", "type", "rule_en", "rule_kr", "example"],
            top=5,
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            score = result.get("@search.score") or 0.0
            examples = result.get("example", [])
            example_str = ", ".join(examples) if examples else "ì˜ˆì‹œ ì—†ìŒ"
            context = f"[Context: {result.get('category', 'N/A')} {result.get('type', 'N/A')} Rule (Score:{score:.2f})] **ê·œì¹™**: {result.get('rule_kr', 'N/A')} **ì˜ˆì‹œ**: {example_str}"
            context_list.append(context)
        return context_list
    except Exception as e:
        logging.error(f"Azure AI Search (Rules Hybrid) ì˜¤ë¥˜: {e}")
        return []


def search_qa_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ Q&A ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=3,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        results = search_client_qa.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["category", "question", "answer"],
            top=3,
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            score = result.get("@search.score") or 0.0
            context = f"[Context: QA-{result.get('category', 'N/A')} (Score:{score:.2f})] **ì§ˆë¬¸**: {result.get('question', 'N/A')} **ë‹µë³€**: {result.get('answer', 'N/A')}"
            context_list.append(context)
        return context_list
    except Exception as e:
        logging.error(f"Azure AI Search (QA Hybrid) ì˜¤ë¥˜: {e}")
        return []


def generate_response_with_llm(user_request: str, context_list: list) -> str:
    """ê²€ìƒ‰ëœ Contextë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context_str = "\n".join(context_list)
    system_prompt = f"""
    ë‹¹ì‹ ì€ ì½”ë”© ëª…ëª… ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ìƒˆë¡œìš´ ë³€ìˆ˜ëª…ì´ë‚˜ í•¨ìˆ˜ëª…ì„ ìƒì„±í•˜ê³ ,
    ëª…ëª… ê·œì¹™ ë˜ëŠ” ìš©ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    **ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‚¬í•­:**
    1. ì•„ë˜ 'ê²€ìƒ‰ëœ ê·œì¹™', 'ìš©ì–´ì‚¬ì „', 'Q&A'ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    2. ìƒˆë¡œìš´ ëª…ëª…ì„ ìƒì„±í•  ê²½ìš°, í•´ë‹¹ë˜ëŠ” ê·œì¹™(ì˜ˆ: camelCase, PascalCase, ë™ì‚¬ ì‹œì‘ ë“±)ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    3. ìš©ì–´ì‚¬ì „ì—ì„œ ì°¾ì€ ì˜ë¬¸ ëŒ€ì•ˆì´ë‚˜ ì•½ì–´ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±ëœ ìš©ì–´ì˜ ëª…í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    4. Q&A ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì°¾ì•˜ë‹¤ë©´, í•´ë‹¹ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

    **ê²€ìƒ‰ëœ ê·œì¹™ ë° ìš©ì–´ (Context):**
    ---
    {context_str}
    ---
    """
    try:
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_request},
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"OpenAI ìµœì¢… ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})"


# ======================================================================
# Streamlit UI êµ¬ì„±
# ======================================================================

# Session State ì´ˆê¸°í™”
if "run_rag" not in st.session_state:
    st.session_state.run_rag = False
if "user_input" not in st.session_state:
    st.session_state.user_input = ""
if "is_processing" not in st.session_state:
    st.session_state.is_processing = False
if "show_warning" not in st.session_state:
    st.session_state.show_warning = False
if "history" not in st.session_state:
    st.session_state.history = []
if "show_warning_empty" not in st.session_state:
    st.session_state.show_warning_empty = False
if "current_result" not in st.session_state:
    st.session_state.current_result = None
if "show_result" not in st.session_state:
    st.session_state.show_result = False


# ì½œë°± í•¨ìˆ˜: ì˜ˆì‹œ ì§ˆë¬¸ ì„¤ì •
def set_example_query(query):
    st.session_state.user_input = query
    st.session_state.run_rag = True
    st.session_state.current_result = None


# ì½œë°± í•¨ìˆ˜: RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def start_rag_process():
    if st.session_state.is_processing:
        st.session_state.show_warning = True
        return

    if st.session_state.user_input:
        st.session_state.run_rag = True
        st.session_state.is_processing = True
        st.session_state.current_result = None
    else:
        st.session_state.show_warning_empty = True


# ì½œë°± í•¨ìˆ˜: ê¸°ë¡ëœ ê²°ê³¼ í‘œì‹œ
def load_history_result(history_index):
    st.session_state.current_result = st.session_state.history[history_index]
    st.session_state.user_input = st.session_state.history[history_index]["question"]
    st.session_state.show_result = True


st.set_page_config(page_title="MVP RAG ê¸°ë°˜ ëª…ëª…/ìš©ì–´ ê°€ì´ë“œ", layout="wide")

st.title("ğŸ’¡ RAG ê¸°ë°˜ ëª…ëª…/ìš©ì–´ ìƒì„± ì „ë¬¸ê°€")
st.markdown("---")

# ----------------- ì‚¬ì´ë“œë°” ì„¤ì • -----------------
with st.sidebar:
    st.header("ê°œìš”")
    st.info(
        f"**RAG ê¸°ë°˜ ëª…ëª…/ìš©ì–´ ìƒì„± ì „ë¬¸ê°€**ëŠ” Azure AI Searchì™€ Azure OpenAIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì½”ë”© ëª…ëª… ê·œì¹™ ë„ìš°ë¯¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ê°œë°œìë“¤ì´ ì¼ê´€ëœ ì½”ë”© í‘œì¤€ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…, ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´ëª… ë“±ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤."
    )
    st.markdown("---")

    # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼
    st.header("ì˜ˆì‹œ ì§ˆë¬¸")
    st.button(
        "Java ë³€ìˆ˜ëª… ê·œì¹™ ì§ˆë¬¸(1)",
        on_click=set_example_query,
        args=["Javaì—ì„œ 'ì¬ê³ 'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ëª…ì„ ê·œì¹™ì— ë§ê²Œ ë§Œë“¤ì–´ì¤˜."],
        key="example1",
    )
    st.button(
        "Java ë³€ìˆ˜ëª… ê·œì¹™ ì§ˆë¬¸(2)",
        on_click=set_example_query,
        args=["Javaì—ì„œ ë°°ì—´ ë³€ìˆ˜ëª…ìœ¼ë¡œ user_listëŠ” ê·œì¹™ì— ë§ë‚˜ìš”?"],
        key="example2",
    )
    st.button(
        "DB ì¸ë±ìŠ¤ ê·œì¹™ ì§ˆë¬¸",
        on_click=set_example_query,
        args=["ë‘ ê°œì˜ ì»¬ëŸ¼ì— ê±¸ì¹œ ë³µí•© ì¸ë±ìŠ¤ë¥¼ ëª…ëª…í•˜ëŠ” ê·œì¹™ì„ ì•Œë ¤ì¤˜."],
        key="example3",
    )
    st.button(
        "ìš©ì–´ ì •ì˜ ìš”ì²­",
        on_click=set_example_query,
        args=[
            "WebUIì—ì„œ 'ë°°ì†¡ ì¤€ë¹„ì¤‘' ìƒíƒœë¥¼ í‘œì‹œí•˜ëŠ” ë¼ë²¨ì˜ ì ‘ë‘ì–´ì™€ í•´ë‹¹ ìš©ì–´ì˜ ì•½ì–´ë¥¼ ì•Œë ¤ì¤˜."
        ],
        key="example4",
    )

    st.markdown("---")

    # ê²€ìƒ‰ ê¸°ë¡ í‘œì‹œ
    st.header("ê²€ìƒ‰ ê¸°ë¡")
    if st.session_state.history:
        for i, item in enumerate(st.session_state.history[::-1]):
            actual_index = len(st.session_state.history) - 1 - i
            st.button(
                f"ğŸ“ {item['question'][:30]}...",
                key=f"hist_{actual_index}",
                on_click=load_history_result,
                args=[actual_index],
            )
    else:
        st.info("ì•„ì§ ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

# ----------------- ë©”ì¸ ì˜ì—­ ì„¤ì • -----------------

user_input_area = st.text_area(
    "ëª…ëª… ê·œì¹™, ìš©ì–´ ì •ì˜ ë˜ëŠ” ìƒˆë¡œìš´ ìš©ì–´ ìƒì„±ì„ ìš”ì²­í•˜ì„¸ìš”:",
    key="user_input",
    placeholder="ì˜ˆ: 'ì¬ê³ 'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ëª…ì„ Java ê·œì¹™ì— ë§ê²Œ ë§Œë“¤ì–´ì£¼ê³ , 'ì¬ê³ 'ì˜ ì•½ì–´ëŠ” ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì¤˜.",
    height=150,
)

run_button = st.button(
    "ì „ë¬¸ê°€ ë‹µë³€ ìƒì„±",
    type="primary",
    disabled=st.session_state.is_processing,
    on_click=start_rag_process,
)

# ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ
if st.session_state.get("show_warning"):
    st.warning("ì´ë¯¸ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ ì£¼ì‹­ì‹œì˜¤.", icon="â³")
    st.session_state.show_warning = False

if st.session_state.get("show_warning_empty"):
    st.warning("ìš”ì²­ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.", icon="â—")
    st.session_state.show_warning_empty = False


# RAG ì‹¤í–‰ ë¡œì§
if st.session_state.run_rag and st.session_state.user_input:
    with st.spinner("ì „ë¬¸ê°€ ë‹µë³€ê³¼ Contextë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            final_user_input = st.session_state.user_input

            # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_list, search_query = extract_keywords_with_llm(final_user_input)

            # 2. Context ê²€ìƒ‰
            rules_context = search_rules_for_context(final_user_input, search_query)
            dictionary_context = search_dictionary_for_terms(
                final_user_input, search_query
            )
            qa_context = search_qa_for_context(final_user_input, search_query)

            # 3. ëª¨ë“  Context í†µí•©
            all_context = rules_context + dictionary_context + qa_context

            if not all_context:
                final_answer = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ëª…ëª… ê·œì¹™ì´ë‚˜ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            else:
                # 4. ìµœì¢… ì‘ë‹µ ìƒì„±
                final_answer = generate_response_with_llm(final_user_input, all_context)

            # 5. ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            result_data = {
                "question": final_user_input,
                "answer": final_answer,
                "metadata": {
                    "ê²€ìƒ‰_ì¿¼ë¦¬": search_query,
                    "í‚¤ì›Œë“œ": keywords_list,
                    "ì´_ê²€ìƒ‰ëœ_Context_ìˆ˜": len(all_context),
                },
                "rules_context": "\n".join(rules_context) if rules_context else "",
                "dictionary_context": (
                    "\n".join(dictionary_context) if dictionary_context else ""
                ),
                "qa_context": "\n".join(qa_context) if qa_context else "",
            }

            # 6. ê¸°ë¡ì— ì¶”ê°€ ë° í˜„ì¬ ê²°ê³¼ ì„¤ì •
            st.session_state.history.append(result_data)
            st.session_state.current_result = result_data
            st.session_state.show_result = True

        except Exception as e:
            st.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}")
        finally:
            # ì²˜ë¦¬ ì™„ë£Œ í›„ í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.is_processing = False
            st.session_state.run_rag = False
            # â­ rerun()ì„ í˜¸ì¶œí•˜ì—¬ ì‚¬ì´ë“œë°” ì—…ë°ì´íŠ¸ ë° ê²°ê³¼ í‘œì‹œ
            st.rerun()


# ê²°ê³¼ í‘œì‹œ (rerun í›„ì—ë„ ìœ ì§€ë¨)
if st.session_state.show_result and st.session_state.current_result:
    result = st.session_state.current_result

    st.success("âœ¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
    st.markdown("### ğŸ’¬ ìµœì¢… ë‹µë³€")
    st.info(result["answer"])

    # â­ ëª¨ë“  Context ì •ë³´ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ Expander ë‚´ë¶€ì— ë°°ì¹˜
    with st.expander("ğŸ” ìƒì„¸ ê²€ìƒ‰ Context ë° ë©”íƒ€ë°ì´í„°", expanded=False):
        tab1, tab2, tab3, tab4 = st.tabs(
            ["ğŸ’¡ ìš”ì•½ ì •ë³´", "1. ëª…ëª… ê·œì¹™", "2. ìš©ì–´ì‚¬ì „", "3. Q&A"]
        )
        with tab1:
            st.markdown("##### ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°")
            st.json(result["metadata"])
        with tab2:
            st.markdown("##### Rules Index (Hybrid Search ê²°ê³¼)")
            st.code(
                result["rules_context"] if result["rules_context"] else "Context ì—†ìŒ",
                language="markdown",
            )
        with tab3:
            st.markdown("##### Dictionary Index (Hybrid Search ê²°ê³¼)")
            st.code(
                (
                    result["dictionary_context"]
                    if result["dictionary_context"]
                    else "Context ì—†ìŒ"
                ),
                language="markdown",
            )
        with tab4:
            st.markdown("##### Q&A Index (Hybrid Search ê²°ê³¼)")
            st.code(
                result["qa_context"] if result["qa_context"] else "Context ì—†ìŒ",
                language="markdown",
            )
