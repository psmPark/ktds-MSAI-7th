import streamlit as st
import os
import logging
from dotenv import load_dotenv

# ======================================================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ======================================================================

# Azure ì„œë¹„ìŠ¤ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)
from openai import AzureOpenAI
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType, VectorizedQuery
from azure.core.credentials import AzureKeyCredential

# ======================================================================
# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ======================================================================
load_dotenv()
# ë¡œê¹… ì„¤ì • (INFO/DEBUG ëŒ€ì‹  WARNING ì´ìƒë§Œ ê¸°ë¡)
logging.basicConfig(
    level=logging.WARNING, format="%(asctime)s - %(levelname)s - %(message)s"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
OPENAI_ENDPOINT = os.getenv("OPENAI_ENDPOINT")
OPENAI_KEY = os.getenv("OPENAI_KEY")
OPENAI_DEPLOYMENT_MODEL = os.getenv("OPENAI_DEPLOYMENT_MODEL")
OPENAI_DEPLOYMENT_EMBEDDING = os.getenv("OPENAI_DEPLOYMENT_EMBEDDING")
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_INDEX_NAME_RULES = os.getenv(
    "AZURE_SEARCH_INDEX_NAME_RULES"
)  # ëª…ëª… ê·œì¹™ ì¸ë±ìŠ¤
AZURE_SEARCH_INDEX_NAME_QA = os.getenv("AZURE_SEARCH_INDEX_NAME_QA")  # Q&A ì¸ë±ìŠ¤
AZURE_SEARCH_INDEX_NAME_DICT = os.getenv(
    "AZURE_SEARCH_INDEX_NAME_DICT"
)  # ìš©ì–´ì‚¬ì „ ì¸ë±ìŠ¤

# 2.1. í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì‚¬
if not all(
    [
        OPENAI_ENDPOINT,
        OPENAI_KEY,
        AZURE_SEARCH_ENDPOINT,
        AZURE_SEARCH_API_KEY,
        OPENAI_DEPLOYMENT_EMBEDDING,
    ]
):
    st.error(
        "í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ (OpenAI/Search)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤."
    )
    st.stop()

# 2.2. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    # Azure OpenAI í´ë¼ì´ì–¸íŠ¸
    openai_client = AzureOpenAI(
        api_key=OPENAI_KEY,
        azure_endpoint=OPENAI_ENDPOINT,
        api_version="2024-12-01-preview",
    )
    # Azure Search ì¸ì¦ ì •ë³´
    search_credential = AzureKeyCredential(AZURE_SEARCH_API_KEY)

    # Azure Search ì¸ë±ìŠ¤ë³„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (3ê°€ì§€ ì¸ë±ìŠ¤)
    search_client_rules = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_RULES, search_credential
    )
    search_client_qa = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_QA, search_credential
    )
    search_client_dict = SearchClient(
        AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME_DICT, search_credential
    )

except Exception as e:
    st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()


# ======================================================================
# 3. RAG íŒŒì´í”„ë¼ì¸ í•µì‹¬ í•¨ìˆ˜
# ======================================================================


def generate_embedding(text: str) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ Azure OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        response = openai_client.embeddings.create(
            input=text, model=OPENAI_DEPLOYMENT_EMBEDDING
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"ì„ë² ë”© API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []


def extract_keywords_with_llm(user_request: str) -> tuple:
    """GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì™€ ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # ... (í”„ë¡¬í”„íŠ¸ ë‚´ìš©)
        prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì—ì„œ ëª…ëª… ê·œì¹™ ë° ìš©ì–´ ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œ(Key Term)ì™€ ì¹´í…Œê³ ë¦¬(Java, Database, UI, Python)ë¥¼
        ìµœëŒ€ 5ê°œê¹Œì§€ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”. ìš”ì²­: {user_request} ->
        """
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant for keyword extraction.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,  # ì¶”ì¶œ ì‘ì—…ì´ë¯€ë¡œ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
        )
        keywords_str = response.choices[0].message.content.strip()
        keywords_list = [k.strip() for k in keywords_str.split(",") if k.strip()]
        search_query = " OR ".join(
            keywords_list
        )  # í‚¤ì›Œë“œë¥¼ ORë¡œ ì—°ê²°í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        return keywords_list, search_query
    except Exception as e:
        logging.error(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [user_request], user_request  # ì˜¤ë¥˜ ì‹œ ì „ì²´ ìš”ì²­ì„ í‚¤ì›Œë“œë¡œ ì‚¬ìš©


def search_dictionary_for_terms(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì‚¬ì „ ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            # ë²¡í„° ê²€ìƒ‰ ì„¤ì • (K-NN)
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        # Azure AI Search ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
        results = search_client_dict.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["korean", "english", "abbreviation", "description"],
            top=5,
            query_type=QueryType.FULL,
        )

        dictionary_context = []
        for result in results:
            score = result.get("@search.score") or 0.0
            # Context ë¬¸ìì—´ í¬ë§·íŒ…
            context = f"[Context: ìš©ì–´ì‚¬ì „(Score:{score:.2f})] **í•œêµ­ì–´**: {result.get('korean', 'N/A')} **ì˜ë¬¸**: {result.get('english', 'N/A')} **ì•½ì–´**: {result.get('abbreviation', 'N/A')} **ì„¤ëª…**: {result.get('description', 'N/A')}"
            dictionary_context.append(context)
        return dictionary_context
    except Exception as e:
        logging.error(f"Azure AI Search (Dictionary Hybrid) ì˜¤ë¥˜: {e}")
        return []


def search_rules_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ëª…ëª… ê·œì¹™ ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            # ë²¡í„° ê²€ìƒ‰ ì„¤ì • (K-NN)
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=5,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        # Azure AI Search ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
        results = search_client_rules.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["category", "type", "rule_en", "rule_kr", "example"],
            top=5,
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            score = result.get("@search.score") or 0.0
            examples = result.get("example", [])
            example_str = ", ".join(examples) if examples else "ì˜ˆì‹œ ì—†ìŒ"
            # Context ë¬¸ìì—´ í¬ë§·íŒ…
            context = f"[Context: {result.get('category', 'N/A')} {result.get('type', 'N/A')} Rule (Score:{score:.2f})] **ê·œì¹™**: {result.get('rule_kr', 'N/A')} **ì˜ˆì‹œ**: {example_str}"
            context_list.append(context)
        return context_list
    except Exception as e:
        logging.error(f"Azure AI Search (Rules Hybrid) ì˜¤ë¥˜: {e}")
        return []


def search_qa_for_context(user_request: str, search_query: str) -> list:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ Q&A ì¸ë±ìŠ¤ì—ì„œ Contextë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_vector = generate_embedding(user_request)

        vector_queries = []
        if query_vector:
            # ë²¡í„° ê²€ìƒ‰ ì„¤ì • (K-NN)
            vector_queries = [
                VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=3,
                    fields="vector_embedding",
                    exhaustive=False,
                )
            ]

        # Azure AI Search ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
        results = search_client_qa.search(
            search_text=search_query,
            vector_queries=vector_queries,
            select=["category", "question", "answer"],
            top=3,
            query_type=QueryType.FULL,
        )

        context_list = []
        for result in results:
            score = result.get("@search.score") or 0.0
            # Context ë¬¸ìì—´ í¬ë§·íŒ…
            context = f"[Context: QA-{result.get('category', 'N/A')} (Score:{score:.2f})] **ì§ˆë¬¸**: {result.get('question', 'N/A')} **ë‹µë³€**: {result.get('answer', 'N/A')}"
            context_list.append(context)
        return context_list
    except Exception as e:
        logging.error(f"Azure AI Search (QA Hybrid) ì˜¤ë¥˜: {e}")
        return []


def generate_response_with_llm(user_request: str, context_list: list) -> str:
    """ê²€ìƒ‰ëœ Contextë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context_str = "\n".join(context_list)
    system_prompt = f"""
    ë‹¹ì‹ ì€ ì½”ë”© ëª…ëª… ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ìƒˆë¡œìš´ ë³€ìˆ˜ëª…ì´ë‚˜ í•¨ìˆ˜ëª…ì„ ìƒì„±í•˜ê³ ,
    ëª…ëª… ê·œì¹™ ë˜ëŠ” ìš©ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    **ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‚¬í•­:**
    1. ì•„ë˜ 'ê²€ìƒ‰ëœ ê·œì¹™', 'ìš©ì–´ì‚¬ì „', 'Q&A'ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    2. ìƒˆë¡œìš´ ëª…ëª…ì„ ìƒì„±í•  ê²½ìš°, í•´ë‹¹ë˜ëŠ” ê·œì¹™(ì˜ˆ: camelCase, PascalCase, ë™ì‚¬ ì‹œì‘ ë“±)ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    3. ìš©ì–´ì‚¬ì „ì—ì„œ ì°¾ì€ ì˜ë¬¸ ëŒ€ì•ˆì´ë‚˜ ì•½ì–´ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±ëœ ìš©ì–´ì˜ ëª…í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    4. Q&A ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì°¾ì•˜ë‹¤ë©´, í•´ë‹¹ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

    **ê²€ìƒ‰ëœ ê·œì¹™ ë° ìš©ì–´ (Context):**
    ---
    {context_str}
    ---
    """
    try:
        # ìµœì¢… ë‹µë³€ ìƒì„±
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_request},
            ],
            temperature=0.3,  # ë‹µë³€ ìƒì„±ì— ì í•©í•œ ì˜¨ë„ ì„¤ì •
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"OpenAI ìµœì¢… ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})"


# ======================================================================
# 4. íŒŒì¼ ë¶„ì„ ë° ì½”ë“œ ê²€í†  ê¸°ëŠ¥ í•¨ìˆ˜
# ======================================================================
def analyze_code_with_llm(file_name: str, code_content: str, context_list: list) -> str:
    """ì—…ë¡œë“œëœ ì½”ë“œ ë‚´ìš©ê³¼ ê·œì¹™ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª…ëª… ê·œì¹™ ìœ„ë°˜ì„ ë¶„ì„í•©ë‹ˆë‹¤."""

    # 1. íŒŒì¼ ìœ í˜• íŒŒì•…
    file_type = file_name.split(".")[-1].upper() if "." in file_name else "UNKNOWN"

    # 2. Context í†µí•©
    context_str = "\n".join(context_list)

    # 3. ì½”ë“œ ë‚´ìš©ì— ë¼ì¸ ë²ˆí˜¸ ì¶”ê°€ (LLMì´ ìœ„ë°˜ ë¼ì¸ì„ ì •í™•íˆ ì§€ëª©í•˜ë„ë¡ ë•ê¸° ìœ„í•¨)
    numbered_content = "\n".join(
        f"{i+1:04d}: {line}" for i, line in enumerate(code_content.splitlines())
    )

    system_prompt = f"""
    ë‹¹ì‹ ì€ ì½”ë”© ëª…ëª… ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ '{file_name}' íŒŒì¼ì˜ '{file_type}' ì½”ë“œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬,
    **ë°˜ë“œì‹œ** ì•„ë˜ 'ê²€ìƒ‰ëœ ê·œì¹™ ë° ìš©ì–´'ë¥¼ ì°¸ê³ í•˜ì—¬ ëª…ëª… ê·œì¹™ì„ ìœ„ë°˜í•œ ëª¨ë“  ëª…ì¹­(ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…, DB ê°ì²´ëª… ë“±)ì„ ì°¾ìœ¼ì„¸ìš”.

    **ì‘ë‹µ í˜•ì‹:**
    1. íŒŒì¼ëª…ê³¼ ë¶„ì„ ìš”ì•½ (ë¶„ì„ëœ ëª…ì¹­ ì´ ê°œìˆ˜, ìœ„ë°˜ ëª…ì¹­ ê°œìˆ˜ ë“±)ì„ ë¨¼ì € ì‘ì„±í•˜ì„¸ìš”.
    2. ë°œê²¬ëœ ëª¨ë“  ìœ„ë°˜ ì‚¬í•­ì„ **ì½”ë“œ ë¼ì¸ ë²ˆí˜¸**ì™€ í•¨ê»˜ í‘œ(Markdown Table)ë¡œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.
    3. í‘œ ì»¬ëŸ¼: ìœ„ë°˜ ëª…ì¹­ | ë¼ì¸ ë²ˆí˜¸ | ìœ„ë°˜ ê·œì¹™ ìœ í˜• | ë°œê²¬ëœ ë¬¸ì œ | ì œì•ˆ ìˆ˜ì •ì•ˆ
    4. ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ëª…ì¹­ì´ ë„ˆë¬´ ì ë‹¤ë©´, ì™œ ë¶„ì„ì´ ì œí•œë˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.

    **ê²€ìƒ‰ëœ ê·œì¹™ ë° ìš©ì–´ (Context):**
    ---
    {context_str}
    ---

    **ë¶„ì„í•  ì½”ë“œ ë‚´ìš© (ë¼ì¸ ë²ˆí˜¸ í¬í•¨):**
    ---
    {numbered_content}
    ---
    """

    try:
        # ì½”ë“œ ë¶„ì„ ìš”ì²­
        response = openai_client.chat.completions.create(
            model=OPENAI_DEPLOYMENT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"'{file_name}' íŒŒì¼ì— ëŒ€í•œ ëª…ëª… ê·œì¹™ ìœ„ë°˜ ë¶„ì„ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”.",
                },
            ],
            temperature=0.1,  # ë¶„ì„ ì •í™•ë„ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"OpenAI ì½”ë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return f"ì½”ë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})"


# ======================================================================
# 5. Streamlit UI êµ¬ì„± ë° í†µí•© ë¡œì§
# ======================================================================

# 5.1. Session State ì´ˆê¸°í™”
if "run_rag" not in st.session_state:
    st.session_state.run_rag = False
if "user_input" not in st.session_state:
    st.session_state.user_input = ""
if "is_processing" not in st.session_state:
    st.session_state.is_processing = False
if "show_warning" not in st.session_state:
    st.session_state.show_warning = False
if "history" not in st.session_state:
    st.session_state.history = []
if "show_warning_empty" not in st.session_state:
    st.session_state.show_warning_empty = False
if "current_result" not in st.session_state:
    st.session_state.current_result = None
if "show_result" not in st.session_state:
    st.session_state.show_result = False
if "uploaded_file" not in st.session_state:  # íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì¶”ê°€
    st.session_state.uploaded_file = None


# 5.2. ì½œë°± í•¨ìˆ˜ ì •ì˜
def start_integrated_process(uploaded_file):
    """RAG íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰ (í…ìŠ¤íŠ¸ ë˜ëŠ” íŒŒì¼) ì½œë°± í•¨ìˆ˜."""
    if st.session_state.is_processing:
        st.session_state.show_warning = True
        return

    # í…ìŠ¤íŠ¸ ì…ë ¥ ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í•¨
    if st.session_state.user_input or uploaded_file:
        st.session_state.run_rag = True
        st.session_state.is_processing = True
        st.session_state.current_result = None
        # íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° íŒŒì¼ ê°ì²´ë¥¼ ì„¸ì…˜ì— ì €ì¥
        if uploaded_file:
            st.session_state.uploaded_file = uploaded_file
    else:
        st.session_state.show_warning_empty = True


def set_example_query(query):
    """ì˜ˆì‹œ ì§ˆë¬¸ì„ ì…ë ¥ í•„ë“œì— ì„¤ì •í•˜ê³  RAG ì‹¤í–‰ì„ ì¤€ë¹„í•˜ëŠ” ì½œë°± í•¨ìˆ˜."""
    st.session_state.user_input = query
    st.session_state.run_rag = True
    st.session_state.is_processing = True
    st.session_state.current_result = None


def load_history_result(history_index):
    """ê²€ìƒ‰ ê¸°ë¡ì˜ ê²°ê³¼ë¥¼ ë©”ì¸ í™”ë©´ì— ë¡œë“œí•˜ì—¬ í‘œì‹œí•˜ëŠ” ì½œë°± í•¨ìˆ˜."""
    st.session_state.current_result = st.session_state.history[history_index]
    st.session_state.user_input = st.session_state.history[history_index]["question"]
    st.session_state.show_result = True


# 5.3. í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ë° ì œëª© ì„¤ì •
st.set_page_config(page_title="AI ì½”ë”© ë„¤ì´ë° ê°€ì´ë“œ", layout="wide")

st.title("ğŸ’¡ AI ì½”ë”© ë„¤ì´ë° ê°€ì´ë“œ")
st.markdown("---")

# 5.4. ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.header("ê°œìš”")
    st.info(
        f"**AI ì½”ë”© ë„¤ì´ë° ê°€ì´ë“œ **ëŠ” Azure AI Searchì™€ Azure OpenAIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì½”ë”© ëª…ëª… ê·œì¹™ ë„ìš°ë¯¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ê°œë°œìë“¤ì´ ì¼ê´€ëœ ì½”ë”© í‘œì¤€ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…, ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´ëª… ë“±ì„ ìƒì„±í•˜ê³  **íŒŒì¼ ë¶„ì„**ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤."
    )
    st.markdown("---")

    # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ ì„¹ì…˜
    st.header("ì˜ˆì‹œ ì§ˆë¬¸")
    st.button(
        "Java ë³€ìˆ˜ëª… ê·œì¹™ ì§ˆë¬¸(1)",
        on_click=set_example_query,
        args=["Javaì—ì„œ 'ì¬ê³ 'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ëª…ì„ ê·œì¹™ì— ë§ê²Œ ë§Œë“¤ì–´ì¤˜."],
        key="example1",
    )
    st.button(
        "Java ë³€ìˆ˜ëª… ê·œì¹™ ì§ˆë¬¸(2)",
        on_click=set_example_query,
        args=["Javaì—ì„œ ë°°ì—´ ë³€ìˆ˜ëª…ìœ¼ë¡œ user_listëŠ” ê·œì¹™ì— ë§ë‚˜ìš”?"],
        key="example2",
    )
    st.button(
        "DB ì¸ë±ìŠ¤ ê·œì¹™ ì§ˆë¬¸",
        on_click=set_example_query,
        args=["ë‘ ê°œì˜ ì»¬ëŸ¼ì— ê±¸ì¹œ ë³µí•© ì¸ë±ìŠ¤ë¥¼ ëª…ëª…í•˜ëŠ” ê·œì¹™ì„ ì•Œë ¤ì¤˜."],
        key="example3",
    )
    st.button(
        "ìš©ì–´ ì •ì˜ ìš”ì²­",
        on_click=set_example_query,
        args=[
            "WebUIì—ì„œ 'ë°°ì†¡ ì¤€ë¹„ì¤‘' ìƒíƒœë¥¼ í‘œì‹œí•˜ëŠ” ë¼ë²¨ì˜ ì ‘ë‘ì–´ì™€ í•´ë‹¹ ìš©ì–´ì˜ ì•½ì–´ë¥¼ ì•Œë ¤ì¤˜."
        ],
        key="example4",
    )
    st.markdown("---")

    # ê²€ìƒ‰ ê¸°ë¡ í‘œì‹œ ì„¹ì…˜
    st.header("ê²€ìƒ‰ ê¸°ë¡")
    if st.session_state.history:
        for i, item in enumerate(st.session_state.history[::-1]):
            actual_index = len(st.session_state.history) - 1 - i
            st.button(
                f"ğŸ“ {item['question'][:30]}...",
                key=f"hist_{actual_index}",
                on_click=load_history_result,
                args=[actual_index],
            )
    else:
        st.info("ì•„ì§ ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

# ======================================================================
# 5.5. ë©”ì¸ ì˜ì—­ - ì‚¬ìš©ì ì…ë ¥ ë° ë²„íŠ¼ (ê°œì„ ëœ íƒ­ UI ì ìš©)
# ======================================================================

# 1, 2ë²ˆ ê¸°ëŠ¥ì„ íƒ­ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬í˜„ (ì‹œê°ì  ê°œì„ )
tab_text, tab_file = st.tabs(["ğŸ’¬ ë„¤ì´ë° & ìš©ì–´ ì§ˆë¬¸í•˜ê¸°", "ğŸ” ì½”ë“œ ëª…ëª… ê·œì¹™ ë¶„ì„"])

with tab_text:
    # í…ìŠ¤íŠ¸ ìš”ì²­ ì˜ì—­
    user_input_area = st.text_area(
        "ëª…ëª… ê·œì¹™, ìš©ì–´ ì •ì˜ ë˜ëŠ” ìƒˆë¡œìš´ ìš©ì–´ ìƒì„±ì„ ìš”ì²­í•˜ì„¸ìš”:",
        key="user_input",
        placeholder="ì˜ˆ: 'ì¬ê³ 'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ëª…ì„ Java ê·œì¹™ì— ë§ê²Œ ë§Œë“¤ì–´ì£¼ê³ , 'ì¬ê³ 'ì˜ ì•½ì–´ëŠ” ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì¤˜.",
        height=100,
        label_visibility="collapsed",  # íƒ­ ë‚´ì—ì„œ ë¼ë²¨ì„ ìˆ¨ê²¨ ê³µê°„ ì ˆì•½
    )

with tab_file:
    # íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­
    st.markdown("ê·œì¹™ ë¶„ì„í•  ì½”ë“œ íŒŒì¼ (Java, Python, SQL/DDL ë“±)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    uploaded_file = st.file_uploader(
        "íŒŒì¼ ì„ íƒ:",
        type=["java", "py", "sql", "ddl", "txt"],
        key="file_uploader_key",
        label_visibility="collapsed",  # ë¼ë²¨ì„ ìˆ¨ê²¨ ê³µê°„ ì ˆì•½
    )
    st.info(
        "íŒŒì¼ ë¶„ì„ ìš”ì²­ ì‹œ, í…ìŠ¤íŠ¸ ìš”ì²­ ë‚´ìš©(1ë²ˆ íƒ­)ì€ ë¶„ì„ì„ ë³´ì¡°í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )


# í†µí•© ì‹¤í–‰ ë²„íŠ¼ì€ íƒ­ ì™¸ë¶€ì— ë°°ì¹˜í•˜ì—¬, ë‘ ì…ë ¥ ì¤‘ í•˜ë‚˜ê°€ ë“¤ì–´ì˜¤ë©´ ë™ì‘í•˜ë„ë¡ í•¨
run_button = st.button(
    "ì „ë¬¸ê°€ ë‹µë³€ ìƒì„± / ì½”ë“œ ë¶„ì„ ì‹œì‘",
    type="primary",
    disabled=st.session_state.is_processing,
    on_click=start_integrated_process,
    args=[uploaded_file],  # íŒŒì¼ ê°ì²´ë¥¼ ì½œë°± í•¨ìˆ˜ì— ì „ë‹¬
)


# ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼)
# ...
# ======================================================================
# 6. RAG ì‹¤í–‰ ë¡œì§ (í†µí•© ì²˜ë¦¬: ì¼ë°˜ ì§ˆì˜ ì‘ë‹µ ë° íŒŒì¼ ë¶„ì„)
# ======================================================================
if st.session_state.run_rag and (
    st.session_state.user_input or st.session_state.get("uploaded_file")
):

    final_user_input = st.session_state.user_input
    file_to_analyze = st.session_state.get("uploaded_file")

    is_file_analysis = file_to_analyze is not None

    if is_file_analysis:
        # --- 6.1. íŒŒì¼ ë¶„ì„ ëª¨ë“œ ---
        with st.spinner(
            f"'{file_to_analyze.name}' íŒŒì¼ì˜ ëª…ëª… ê·œì¹™ ìœ„ë°˜ ì‚¬í•­ê³¼ ìš©ì–´ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        ):
            try:
                # 1. íŒŒì¼ ë‚´ìš© ì½ê¸°
                code_content = file_to_analyze.read().decode("utf-8")

                # 2. íŒŒì¼ íƒ€ì… ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ/Context ê²€ìƒ‰ì„ ìœ„í•œ ìš”ì²­ í…ìŠ¤íŠ¸ ìƒì„±
                file_ext = (
                    file_to_analyze.name.split(".")[-1].upper()
                    if "." in file_to_analyze.name
                    else "UNKNOWN"
                )
                analysis_request_text = f"ê·œì¹™ ë¶„ì„ ìš”ì²­: {file_to_analyze.name} íŒŒì¼ ({file_ext})ì˜ ëª…ëª…, ìš©ì–´, ê´€í–‰"

                # í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                _, search_query = extract_keywords_with_llm(analysis_request_text)

                # 3. Context ê²€ìƒ‰ (3ê°€ì§€ ì¸ë±ìŠ¤ ëª¨ë‘ í™œìš©)
                rules_context = search_rules_for_context(
                    analysis_request_text, search_query
                )
                dictionary_context = search_dictionary_for_terms(
                    analysis_request_text, search_query
                )
                qa_context = search_qa_for_context(analysis_request_text, search_query)

                # ëª¨ë“  Context í†µí•©
                all_context = rules_context + dictionary_context + qa_context

                # 4. LLM í†µë¶„ì„ ìš”ì²­ (ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ)
                final_answer = analyze_code_with_llm(
                    file_to_analyze.name, code_content, all_context
                )

                # 5. ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                result_data = {
                    "question": f"[íŒŒì¼ ë¶„ì„] {file_to_analyze.name} ({file_ext})",
                    "answer": final_answer,
                    "metadata": {
                        "ë¶„ì„_ìœ í˜•": "ì½”ë“œ ëª…ëª… ê·œì¹™ ë¶„ì„ (3ê°œ ì¸ë±ìŠ¤ í™œìš©)",
                        "íŒŒì¼_í¬ê¸°_bytes": len(code_content.encode("utf-8")),
                        "ê²€ìƒ‰_ì¿¼ë¦¬": search_query,
                        "ì´_ê²€ìƒ‰ëœ_Context_ìˆ˜": len(all_context),
                    },
                    "rules_context": (
                        "\n".join(rules_context)
                        if rules_context
                        else "ê·œì¹™ Context ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                    ),
                    "dictionary_context": (
                        "\n".join(dictionary_context)
                        if dictionary_context
                        else "ìš©ì–´ì‚¬ì „ Context ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                    ),
                    "qa_context": (
                        "\n".join(qa_context)
                        if qa_context
                        else "Q&A Context ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                    ),
                }

            except Exception as e:
                st.error(f"íŒŒì¼ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
                st.session_state.is_processing = False
                st.session_state.run_rag = False
                st.stop()

    else:
        # --- 6.2. ì¼ë°˜ ì§ˆì˜ ì‘ë‹µ ëª¨ë“œ ---
        with st.spinner("ì „ë¬¸ê°€ ë‹µë³€ê³¼ Contextë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords_list, search_query = extract_keywords_with_llm(
                    final_user_input
                )

                # 2. Context ê²€ìƒ‰ (3ê°€ì§€ ì¸ë±ìŠ¤)
                rules_context = search_rules_for_context(final_user_input, search_query)
                dictionary_context = search_dictionary_for_terms(
                    final_user_input, search_query
                )
                qa_context = search_qa_for_context(final_user_input, search_query)

                # 3. ëª¨ë“  Context í†µí•©
                all_context = rules_context + dictionary_context + qa_context

                if not all_context:
                    final_answer = (
                        "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ëª…ëª… ê·œì¹™ì´ë‚˜ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
                else:
                    # 4. ìµœì¢… ì‘ë‹µ ìƒì„± (RAG í•¨ìˆ˜ í˜¸ì¶œ)
                    final_answer = generate_response_with_llm(
                        final_user_input, all_context
                    )

                # 5. ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                result_data = {
                    "question": final_user_input,
                    "answer": final_answer,
                    "metadata": {
                        "ë¶„ì„_ìœ í˜•": "ì¼ë°˜ ì§ˆì˜ ì‘ë‹µ",
                        "ê²€ìƒ‰_ì¿¼ë¦¬": search_query,
                        "í‚¤ì›Œë“œ": keywords_list,
                        "ì´_ê²€ìƒ‰ëœ_Context_ìˆ˜": len(all_context),
                    },
                    "rules_context": (
                        "\n".join(rules_context) if rules_context else "Context ì—†ìŒ"
                    ),
                    "dictionary_context": (
                        "\n".join(dictionary_context)
                        if dictionary_context
                        else "Context ì—†ìŒ"
                    ),
                    "qa_context": (
                        "\n".join(qa_context) if qa_context else "Context ì—†ìŒ"
                    ),
                }

            except Exception as e:
                st.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}")
                st.session_state.is_processing = False
                st.session_state.run_rag = False
                st.stop()

    # 6. ê¸°ë¡ì— ì¶”ê°€ ë° í˜„ì¬ ê²°ê³¼ ì„¤ì • (ê³µí†µ)
    st.session_state.history.append(result_data)
    st.session_state.current_result = result_data
    st.session_state.show_result = True

    # 7. ì²˜ë¦¬ ì™„ë£Œ í›„ í”Œë˜ê·¸ ì´ˆê¸°í™”
    st.session_state.is_processing = False
    st.session_state.run_rag = False
    st.session_state.uploaded_file = None  # íŒŒì¼ ë¶„ì„ ì™„ë£Œ í›„ ì´ˆê¸°í™”
    st.rerun()


# ======================================================================
# 7. ê²°ê³¼ í‘œì‹œ ë¡œì§
# ======================================================================
if st.session_state.show_result and st.session_state.current_result:
    result = st.session_state.current_result

    # íŒŒì¼ ë¶„ì„ ê²°ê³¼ëŠ” ë³„ë„ ì œëª© ì‚¬ìš©
    if result["metadata"].get("ë¶„ì„_ìœ í˜•") == "ì½”ë“œ ëª…ëª… ê·œì¹™ ë¶„ì„ (3ê°œ ì¸ë±ìŠ¤ í™œìš©)":
        st.success(
            f"âœ¨ ì½”ë“œ ë¶„ì„ ì™„ë£Œ: {result['question'].replace('[íŒŒì¼ ë¶„ì„] ', '')}"
        )
        st.markdown("### ğŸ“ LLM ì½”ë“œ ë¶„ì„ ê²°ê³¼")
        # LLMì´ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì •ë¦¬í–ˆì„ ê²ƒì„ ê°€ì •í•˜ê³  raw markdownìœ¼ë¡œ ì¶œë ¥
        st.markdown(result["answer"])
    else:
        st.success("âœ¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
        st.markdown("### ğŸ’¬ ìµœì¢… ë‹µë³€")
        st.info(result["answer"])

    # ëª¨ë“  Context ì •ë³´ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ Expander ë‚´ë¶€ì— ë°°ì¹˜
    with st.expander("ğŸ” ìƒì„¸ ê²€ìƒ‰ Context ë° ë©”íƒ€ë°ì´í„°", expanded=False):
        tab1, tab2, tab3, tab4 = st.tabs(
            ["ğŸ’¡ ìš”ì•½ ì •ë³´", "1. ëª…ëª… ê·œì¹™", "2. ìš©ì–´ì‚¬ì „", "3. Q&A"]
        )
        with tab1:
            st.markdown("##### ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°")
            st.json(result["metadata"])
        with tab2:
            st.markdown("##### Rules Index (Hybrid Search ê²°ê³¼)")
            st.code(
                result["rules_context"],
                language="markdown",
            )
        with tab3:
            st.markdown("##### Dictionary Index (Hybrid Search ê²°ê³¼)")
            st.code(
                result["dictionary_context"],
                language="markdown",
            )
        with tab4:
            st.markdown("##### Q&A Index (Hybrid Search ê²°ê³¼)")
            st.code(
                result["qa_context"],
                language="markdown",
            )
